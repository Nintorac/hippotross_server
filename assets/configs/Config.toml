[model]
# backend = "WebGpu"                                     # Backend for inference ("WebGpu" or "Hip"). Omitting defaults to WebGpu.
embed_device = "Cpu"                                   # Device to put the embed tensor ("Cpu" or "Gpu").
max_batch = 8                                          # The maximum batches that are cached on GPU.
name = "rwkv7-g1a-0.1b-20250728-ctx4096.st"            # Name of the model.
path = "assets/models"                                 # Path to the folder containing all models.
precision = "Fp16"                                     # Precision for intermediate tensors ("Fp16" or "Fp32"). "Fp32" yields better outputs but slower.
quant = 0                                              # Layers to be quantized.
quant_type = "Int8"                                    # Quantization type ("Int8" or "NF4").
stop = ["\n\n"]                                        # Additional stop words in generation.
token_chunk_size = 256                                 # Size of token chunk that is inferred at once. For high end GPUs, this could be 64 to 1024 (faster).

# [[state]] # State-tuned initial state.
# id = "fd7a60ed-7807-449f-8256-bccae3246222"                      # UUID for this state, which is used to specify which one to use in the APIs.
# name = "x060-3B"                                                 # Given name for this state (optional).
# path = "rwkv-x060-chn_single_round_qa-3B-20240505-ctx1024.state"

# [[state]] # Load another initial state.
# id = "6a9c60a4-0f4c-40b1-a31f-987f73e20315"                      # UUID for this state.
# path = "rwkv-x060-chn_single_round_qa-3B-20240502-ctx1024.state"

# [[lora]] # LoRA and blend factors.
# alpha = 192
# path = "assets/models/rwkv-x060-3b.lora"

[tokenizer]
path = "assets/tokenizer/rwkv_vocab_v20230424.json" # Path to the tokenizer.

[bnf]
enable_bytes_cache = true   # Enable the cache that accelerates the expansion of certain short schemas.
start_nonterminal = "start" # The initial nonterminal of the BNF schemas.

[adapter]
Auto = {} # Choose the best GPU.
# Manual = 0 # Manually specify which GPU to use.

[listen]
acme = false
domain = "local"
ip = "0.0.0.0"   # Use IpV4.
# ip = "::"        # Use IpV6.
force_pass = true
port = 65530
slot = "permisionkey"
tls = false

[[listen.app_keys]] # Allow mutiple app keys.
app_id = "admin"
secret_key = "ai00_is_good"

[web] # Remove this to disable WebUI.
path = "assets/www/index.zip" # Path to the WebUI.

# [embed] # Uncomment to enable embed models (via fast-embedding onnx models).
# endpoint = "https://hf-mirror.com"
# home = "assets/models/hf"
# lib = "assets/ort/onnxruntime.dll"  # Only used under windows.
# name = { MultilingualE5Small = {} }

# [prompts] # Uncomment to customize prompts. Defaults shown below.
# Tool system prompt configuration (Hermes/Qwen style)
# tool_header = "\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n"
# tool_footer = "</tools>\n\nIMPORTANT: To call a function, use EXACTLY this XML format:\n<tool_call>\n{\"name\": \"function_name\", \"arguments\": {\"param\": \"value\"}}\n</tool_call>\n\nNEVER use <tool_use>. ALWAYS use <tool_call> with \"arguments\" field."
# Thinking mode suffixes (appended to user message based on budget_tokens)
# thinking_suffix_short = " think a bit"       # tier 1: 1024-4095 tokens
# thinking_suffix_standard = " think"          # tier 2: 4096-16383 tokens
# thinking_suffix_extended = " think a lot"    # tier 3+: 16384+ tokens
# Role names for prompt formatting
# role_user = "User"
# role_assistant = "A"
# role_system = "System"
# Assistant generation prefixes
# assistant_prefix = "A:"
# assistant_prefix_thinking = "A: <think"
# Default stop sequences (when not provided in request)
# default_stop_sequences = ["\n\nUser:"]
